---
title: "Web Scraping with R"
output:
  html_document:
    df_print: paged
---

## Web Scraping Lots of Data For The Taking ?

The web hosts lots of interesting data that you can ”scrape”. Some of it is stashed in data bases, behind APIs, or in free form text. Lots of people want to grab information of of Twitter or from user forums to see what people are thinking. There is a lot of valuable information out there for the taking although some web sites have “caught on” and either block programmatic access or they setup “pay walls” that require you to subscribe to an API for access. The New York Times does this. But there are lots of opportunities to get data.

Depending on what web sites you want to scrape the process can be involved and quite tedious. Many websites are very much aware that people are scraping so they offer Application Programming Interfaces (APIs) to make requests for information easier for the user and easier for the server administrators to control access. Most times the user must apply for a “key” to gain access.

For premium sites, the key costs money. Some sites like Google and Wunderground (a popular weather site) allow some number of free accesses before they start charging you. Even so the results are typically returned in XML or JSON which then requires you to parse the result to get the information you want. In the best situation there is an R package that will wrap in the parsing and will return lists or data frames.

See https://en.wikipedia.org/wiki/Web_scraping
See https://medium.com/@tjwaterman99/web-scraping-is-now-legal-6bf0e5730a78
See https://www.nytimes.com/
See https://www.insidehighered.com/aaup-compensation-survey


## Web Scraping Can Be Ugly

Here is a summary:

    First. Always try to find an R package that will access a site (e.g. New York Times, Wunderground, PubMed). These packages (e.g. omdbapi, easyPubMed, RBitCoin, rtimes) provide a programmatic search interface and return data frames with little to no effort on your part.

    If no package exists then hopefully there is an API that allows you to query the website and get results back in JSON or XML. I prefer JSON because it’s “easier” and the packages for parsing JSON return lists which are native data structures to R. So you can easily turn results into data frames. You will ususally use the rvest package in conjunction with XML, and the RSJONIO packages.

    If the Web site doesn’t have an API then you will need to scrape text. This isn’t hard but it is tedious. You will need to use rvest to parse HMTL elements. If you want to parse mutliple pages then you will need to use rvest to move to the other pages and possibly fill out forms. If there is a lot of Javascript then you might need to use RSelenium to programmatically manage the web page.

### Understanding The Language of The Web

The Web has its own languages: HTML, CSS, Javascript

```{r eval=FALSE}
<h1>, <h2>, ..., <h6> Heading 1 and so on
<p> Paragraph elements
<ul> Unordered List
<ol> Ordered List
<li> List Element
<div> Division / Section
<table> Tables
<form> Web forms
```

So to be productive at scraping requires you to have some familiarity with HMTL XML, and CSS. Here we look at a very basic HTML file. Refer to See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html for a basic introductory session on HTML and webscraping with R

```{r eval=FALSE}
<!DOCTYPE html>
<html>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
    
     <table>
        <tr><th>First</th><th>Last</th></tr>
        <tr><td>Steve</td><td>Jones</td></tr>
        <tr><td>Mary</td><td>Smith</td></tr>
        <tr><td>Frank</td><td>Collins</td></tr>
    </table>
        
   </body>
</html>
```

And you could apply some styling to this courtest of the CSS language which allows you to inject styles into plain HTML:

```{r eval=FALSE}
<!DOCTYPE html>
<html>
  <head>
<style>
h1 {
  color: red;
}

h2 {
  color: red;
}

p { 
  color: blue;
}

table, th, td {
  border: 1px solid green;
}
</style>
</head>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
    <table>
        <tr><th>First</th><th>Last</th></tr>
        <tr><td>Steve</td><td>Jones</td></tr>
        <tr><td>Mary</td><td>Smith</td></tr>
        <tr><td>Frank</td><td>Collins</td></tr>
    </table>
        
      </th>
    </table>
   </body>
</html>

```

### Useful tools

There are a number of tools that allow us to inspect web pages and see “what is under the hood”. Warning - I just discovered that one of my favorite browser plugins (firebug) to find the xpaths and/or css paths of page elements is no longer supported under Firefox or Chrome. I’ve found a couple of replacements but they don’t work as well. I’ll research it more. The way that Selector Gadget and xPath work is that you install them into your browswer and then activate them whenever you need to identify the selector associated with a part of a web page.

Selector Gadget and xPATH Helper are the two I use the most. Google and Firefox have ways to view the underlying source code but it can be confusing to use that approach. Sometimes, it's the best you have.

## Rvest

Now let’s do a quick rvest tutorial. There are several steps involved in using rvest which are conceptually quite straightforward:

- Identify a URL to be examined for content
- Use Selector Gadet, xPath, or Google Insepct to identify the “selector” This will be a paragraph, table, hyper links, images
- Load rvest
- Use read_html to "read" the URL
- Pass the result to html_nodes to get the selectors identified in 
step number 2
- Get the text or table content

```{r}
library(rvest)
url <- "https://en.wikipedia.org/wiki/World_population"

(paragraphs <- read_html(url) %>% html_nodes("p"))
```

Then we might want to actually parse out those paragraphs into text:

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
paragraphs <- read_html(url) %>% html_nodes("p") %>% html_text()
paragraphs[1:10]
```

Get some other types of HTML objects. Let’s get all the hyperlinks to other pages

```{r}
read_html(url) %>% html_nodes("a") 
```

What about tables ?

```{r}
url <- "https://en.wikipedia.org/wiki/World_population"
tables <- read_html(url) %>% html_nodes("table") 
tables
```

### Example: Parsing A Table From Wikipedia

Look at the Wikipedia Page for world population:

https://en.wikipedia.org/wiki/World_population

    We can get any table we want using rvest
    We might have to experiment to figure out which one
    Get the one that lists the ten most populous countries
    I think this might be the 4th or 5th table on the page
    How do we get this ?

First we will load packages that will help us throughout this session.

In this case we’ll need to figure out what number table it is we want. We could fetch all the tables and then experiment to find the precise one.


```{r}
library(rvest)
library(tidyr)
library(dplyr)
library(ggplot2)

# Use read_html to fetch the webpage
url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 

# Use Selector Gadget or XPATH Helper to find the right table
ten_most_populous <- ten_most_df %>% 
  html_nodes("table") %>% `[[`(5) %>% html_table()

# Let's get just the first three columns
ten_most_populous <- ten_most_populous[,2:4]

# Get some content - Change the column names
names(ten_most_populous) <- c("Country_Territory","Population","Date")

# Do reformatting on the columns to be actual numerics where appropriate
ten_most_populous %>% 
  mutate(Population=gsub(",","",Population)) %>% 
  mutate(Population=round(as.numeric(Population)/1e+06))  %>%
  ggplot(aes(x=Country_Territory,y=Population)) + geom_point() + 
  labs(y = "Population / 1,000,000") + coord_flip() +
  ggtitle("Top 10 Most Populous Countries") + theme_bw()

```


In the above example we leveraged the fact that we were looking specifically for a table element and it became a project to locate the correct table number. This isn’t always the case with more complicated websites in that the element we are trying to grab or scrape is contained within a nested structure that does not correspond neatly to a paragraph, link, heading, or table. This can be the case if the page is heavily styled with CSS or Javascript. We might have to work harder. But it’s okay to try to use simple elements and then try to refine the search some more.

```{r}
# Could have use the xPath plugin to help

url <- "https://en.wikipedia.org/wiki/World_population"
ten_most_df <- read_html(url) 
  
ten_most_populous <- ten_most_df %>% 
  html_nodes(xpath="/html/body/div[3]/div[3]/div[5]/div[1]/table[4]") %>% html_table()

ten_most_populous
```



## Scraping Patient Dialysis Stories

Here is an example relating to the experiences of dialysis patients with a specific dialysis provider. It might be more useful to find a support forum that is managed by dialysis patients to get more general opinions but this example is helpful in showing you what is involved. Check out this website:

```{r}
# https://www.americanrenal.com/dialysis-centers/patient-stories
# https://www.americanrenal.com/dialysis-centers/patient-stories/john-baguchinsky

library(rvest)

burl <- "https://www.americanrenal.com/dialysis-centers/patient-stories"
```

```{r}
# Setup an empty vector to which we will add the content of each story
workVector <- vector()


# Grab the links from the site that relate patient stories

links <- read_html(burl) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>% 
  grep("stories",.,value=TRUE)

links
```

Some of these links do not correspond directly to a specific patient name so we need to filter those out.Get only the ones that seem to have actual names associated with them.

```{r}
storiesLinks <- links[-grep("stories$",links)] 

storiesLinks
```

Next we will visit each of these pages and scrape the text information. We’ll step through this in class so you can see this in action but here is the code. We will get each story and place each paragrpah of the story into a vector element. After that we will eliminate blank lines and some junk lines that begin with a new line character. Then we will collapse all of the vector text into a single paragraph and store it into a list element. Let’s step through it for the first link.


```{r}
# This corresponds to the first link
# "http://www.americanrenal.com/dialysis-centers/patient-stories/randal-beatty" 

tmpResult <- read_html(storiesLinks[1]) %>% 
                       html_nodes("p") %>% html_text()

tmpResult
```

Okay, that has some junk in it like blank lines and lines that begin with new line characters.

```{r}
tmpResult <- tmpResult[tmpResult!=""]
  
  # Get rid of elements that begin with a newline character "\n"
    
    newlines_begin <- sum(grepl("^\n",tmpResult))

    if (newlines_begin > 0) {
      tmpResult <- tmpResult[-grep("^\n",tmpResult)]
    }

    tmpResult
```

Next, let’s create a more compact version of the data. We’ll cram it all into a single element.

```{r}
(tmpResult <- paste(tmpResult,collapse=""))
```

So we could put this logic into a loop and process each of the links programmatically.

```{r}
 # Now go to these pages and scrape the text necessary to
  # build a corpus

  tmpResult <- vector()
  textList <- list()
  
  for (ii in 1:length(storiesLinks)) {
    tmpResult <- read_html(storiesLinks[ii]) %>% 
                       html_nodes("p") %>% html_text()
  
  # Get rid of elements that are a blank line
    
    tmpResult <- tmpResult[tmpResult!=""]
  
  # Get rid of elements that begin with a newline character "\n"
    
    newlines_begin <- sum(grepl("^\n",tmpResult))

    if (newlines_begin > 0) {
      tmpResult <- tmpResult[-grep("^\n",tmpResult)]
    }
      
    # Let's collpase all the elements into a single element and then store
    # it into a list element so we can maintain each patient story separately
    # This is not necessary but until we figure out what we want to do with
    # the data then this gives us some options
    
    tmpResult <- paste(tmpResult,collapse="")
    textList[[ii]] <- tmpResult
  }
```


Summary

    Need some basic HTML and CSS knowledge to find correct elements
    How to extract text from common elements
    How to extract text from specific elements
    Always have to do some text cleanup of data
    It usually takes multiple times to get it right

See http://bradleyboehmke.github.io/2015/12/scraping-html-text.html


## More Examples

Okay. This is a tour of some sites that will serve as important examples on how to parse sites. Let’s check the price of bitcoins. You want to be rich don’t you ?

### Bitcoin

See https://coinmarketcap.com/all/views/all/

The challenge here is that it’s all one big table and it’s not clear how to adress it. And the owners of the web site will ususally change the format or start using Javascript or HTML5 which will mess things up in the future. One solid approach I frequently use is to simply pull out all the tables and, by experimentation, try to figure out which one has the information I want. This always require some work.

```{r}
library(rvest)
url <- "https://coinmarketcap.com/all/views/all/"
bc <- read_html(url)

bc_table <- bc %>% 
  html_nodes('table') %>% 
  html_table() %>% .[[3]]
 # We get back a one element list that is a data frame
 str(bc_table,0)
```

```{r}
bc_table
```

```{r}
bc_table <- bc_table[,c(2:3,5)]
head(bc_table)
```

Everything is a character at this point so we have to go in an do some surgery on the data frame to turn the Price into an actual numeric.

```{r}
# The data is "dirty" and has characers in it that need cleaning
bc_table <- bc_table %>% mutate(Price=gsub("\\$","",Price))
bc_table <- bc_table %>% mutate(Price=gsub(",","",Price))
bc_table <- bc_table %>% mutate(Price=round(as.numeric(Price),2))

# There are four rows wherein the Price is missing NA
bc_table <- bc_table %>% filter(complete.cases(bc_table))

# Let's get the Crypto currencies with the Top 10 highest prices 
top_10 <- bc_table %>% arrange(desc(Price)) %>% head(10)
top_10
```


Let’s make a barplot of the top 10 crypto currencies.

```{r}
# Next we want to make a barplot of the Top 10
ylim=c(0,max(top_10$Price)+10000)
main="Top 10 Crypto Currencies in Terms of Price"
bp <- barplot(top_10$Price,col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

So that didn’t work out so well since one of the crypto currencies dominates the others in terms of price. So let’s create a log transformed verion of the plot.

```{r}
# Let's take the log of the price
ylim=c(0,max(log(top_10$Price))+5)
main="Top 10 Crypto Currencies in Terms of log(Price)"
bp <- barplot(log(top_10$Price),col="aquamarine",
              ylim=ylim,main=main)
axis(1, at=bp, labels=top_10$Symbol,  cex.axis = 0.7)
grid()
```

## PubMed

```{r}
# "hemodialysis, home" [MeSH Terms] 


url<-"https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"

# 
# The results from the search will be of the form:
# https://www.ncbi.nlm.nih.gov/pubmed/30380542

results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/[0-9]{6,6}",.,value=TRUE) %>% unique(.)

results
```

```{r}
text.vec <- vector()

for (ii in 1:length(results)) {
  string <- paste0("https://pubmed.ncbi.nlm.nih.gov",results[ii])
  text.vec[ii] <- read_html(string) %>% html_nodes(xpath="/html/body/div[5]/main/div[2]/div/p") %>%  html_text()
}

# Eliminate lines with newlines characters
final.vec <- gsub("\n","",text.vec)
final.vec <- gsub("^\\s+","",final.vec)

#final.vec <- text.vec[grep("^\n",text.vec,invert=TRUE)]

final.vec
```

Well that was tedious. And we processed only the first page of results. How do we “progrmmatically” hit the “Next” Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn’t obvious how to do this. If we hove over the “Next” button we don’t get an associated link.


## IMDB

Look at this example from IMDb (Internet Movie Database). According to Wikipedia:

IMDb (Internet Movie Database)[2] is an online database of information related to films, television programs, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, fan and critical reviews, and ratings. We can search or refer to specific movies by URL if we wanted. For example, consider the following link to the “Lego Movie”:

```{r eval=FALSE}
http://www.imdb.com/title/tt1490017/
```

In terms of scraping information from this site we could do that using the rvest package. Let’s say that we wanted to capture the rating information which is 7.8 out of 10. We could use the xPath Tool or the Selector gadget tool to zone in on this information. According to selector gadget we have the following xpath expression:

```{r}
url <- "http://www.imdb.com/title/tt1490017/"
lego_movie <- read_html(url)

# Scrape the website for the movie rating
rating <- lego_movie %>%
   html_nodes(xpath="/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[1]/div[2]/div/div[1]/a/div/div/div[2]/div[1]/span[1]")  %>%
#  html_nodes(".ratingValue span") %>%
  html_text() 
rating
```

Could also use CSS which could be more direct though also somewhat confusing to find. I used Google Chrome Inspector to find the class tag for the rating.

```{r}
rating <- lego_movie %>%
   html_nodes(css="span.sc-7ab21ed2-1.jGRxWM")  %>%
#  html_nodes(".ratingValue span") %>%
  html_text() 
rating
```


So that gives us what we need albeit in character form. Now it’s a simple matter of parsing out the first rating value:

```{r}
(rating <- as.numeric(rating))
```

```{r}
# Scrape the website for the movie rating
rating <- lego_movie %>%
  html_nodes('.ratingValue') %>%
  html_text() 

rating
```

Let’s access the summary section of the link. We could use Selector Gadget or the xPath plugin. I’ll use the former.

```{r}
url <- 'https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2'
summary <- read_html(url) %>% 
  html_nodes(xpath="/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[3]/div[2]/div[1]/div[1]/div[2]/span[3]") %>%
  html_text()

summary
```



## PubMed

Pubmed provides a rich source of information on published scientific literature. There are tutorials on how to leverage its capabilities but one thing to consider is that MESH terms are a good starting place since the search is index-based. MeSH (Medical Subject Headings) is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed. It’s faster and more accurate so you can first use the MESH browser to generate the appropriate search terms and add that into the Search interface. The MESH browser can be found at https://www.ncbi.nlm.nih.gov/mesh/

What we do here is get the links associated with each publication so we can then process each of those and get the abstract associated with each publication.

```{r}
# "hemodialysis, home" [MeSH Terms] 

url<-"https://www.ncbi.nlm.nih.gov/pubmed/?term=%22hemodialysis%2C+home%22+%5BMeSH+Terms%5D"

# 
# The results from the search will be of the form:
# https://www.ncbi.nlm.nih.gov/pubmed/30380542

results <- read_html(url) %>% 
  html_nodes("a") %>% 
  html_attr("href") %>%
  grep("/[0-9]{6,6}",.,value=TRUE) %>% unique(.)

results
```

So now we could loop through these links and get the abstracts for these results. It looks that there are approximately 20 results per page. As before we would have to dive in to the underlying structure of the page to get the correct HTML pathnames or we could just look for Paragraph elements and pick out the links that way.

```{r}
text.vec <- vector()

for (ii in 1:length(results)) {
  string <- paste0("https://pubmed.ncbi.nlm.nih.gov",results[ii])
  print(string)
#  text.vec[ii] <- read_html(string) %>% html_nodes("p") %>% `[[`(7) %>% html_text()
  text.vec[ii] <- read_html(string) %>% html_nodes(xpath="/html/body/div[5]/main/div[2]/div/p") %>% html_text()
}

# Eliminate lines with newlines characters

final.vec <- gsub("\n","",text.vec)
final.vec <- gsub("^\\s+","",final.vec)


final.vec
```

Well that was tedious. And we processed only the first page of results. How do we “progrmmatically” hit the “Next” Button at the bottom of the page ? This is complicated by the fact that there appears to be some Javascript at work that we would have to somehow interact with to get the URL for the next page. Unlike with the school salary example it isn’t obvious how to do this. If we hove over the “Next” button we don’t get an associated link.


## APIs

APIs are "Application Programming Interfaces" which allow you to use a high level programming language such as R, Python, Javascript, Java, C++ -or- pretty much any language to access web content. Unlike web scraping, you can use a protocol called REST which allows you to access content via a URL. What you get back is in either XML or JSON. 

https://restfulapi.net/

Let’s look at the IMDB page which catalogues lots of information about movies. Just got to the web site and search although here is an example link. https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2 In this case we would like to get the summary information for the movie. So we would use Selector Gadget or some other method to find the XPATH or CSS associated with this element.

This pretty easy and doesn’t present much of a problem although for large scale mining of movie data we would run into trouble because IMDB doesn’t really like you to scrape their pages. They have an API that they would like for you to use.

```{r}
url <- 'https://www.imdb.com/title/tt0076786/?ref_=fn_al_tt_2'
summary <- read_html(url) %>% 
#  html_nodes(xpath="/html/body/div[2]/main/div/section[1]/section/div[3]/section/section/div[3]/div[2]/div[1]/div[1]/div[2]/span[3]") %>%
  html_nodes(css="span.sc-16ede01-2.gXUyNh") %>%
  html_text()

summary
```

But here we go again. We have to parse the desired elements on this page and then what if we wanted to follow other links or set up a general function to search IMDB for other movies of various genres, titles, directors, etc.

So now we have an alternative to scraping IMDB by using OMDB which strives to provide much in the way of similar information via an API. https://www.omdbapi.com/

So as an example on how this works. Paste the URL into any web browser. You must supply your key for this to work. What you get back is a JSON formatted entry corresponding to ”The GodFather”movie.

```{r eval=FALSE}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Lego+Movie"
```



```{r}
library(RJSONIO)

url <- "http://www.omdbapi.com/?apikey=f7c004c&t=The+Lego+Movie"

# Fetch the URL via fromJSON
movie <- fromJSON("http://www.omdbapi.com/?apikey=f7c004c&t=The+Lego+Movie")

# We get back a list which is much easier to process than raw JSON or XML
str(movie)
```

```{r}
movie$Plot
```

```{r}
sapply(movie$Ratings,unlist)
```

Let’s Get all the Episodes for Season 1 of Game of Thrones

```{r}
url <- "http://www.omdbapi.com/?apikey=f7c004c&t=Game%20of%20Thrones&Season=1"
movie <- fromJSON(url)
str(movie,1)
```

```{r}
episodes <- data.frame(do.call(rbind,movie$Episodes),
                       stringsAsFactors = FALSE)
episodes
```

## OMDB Package

Wait a minute. Looks like someone created an R package that wraps all this for us. It is called omdbapi and this then bypassed the need for scraping or using a REST API. In reality, the package does use an API within the package itself. But the advantage of the package is that it provides convenient functions that handle the API interactions for you.

```{r eval=FALSE}
devtools::install_github("hrbrmstr/omdbapi")
```

```{r}
library(omdbapi)
# The first time you use this you will be prompted to enter your
 # API key
movie_df <- search_by_title("Game of Thrones", page = 2)
(movie_df <- movie_df[,-5])
```

```{r}
(gf <- find_by_title("The Lego Movie"))
```

```{r}
get_actors((gf))
```


##  EasyPubMed

So there is an R package called EasyPubMed that helps ease the access of data on the Internet. The idea behind this package is to be able to query NCBI Entrez and retrieve PubMed records in XML or TXT format.

```{r}
library(easyPubMed)
my_query <- 'hemodialysis, home" [MeSH Terms]'
my_entrez_id <- get_pubmed_ids(my_query)

my_abstracts <- fetch_pubmed_data(my_entrez_id,retmax=200)
my_abstracts <- custom_grep(my_abstracts,"AbstractText","char")


my_abstracts[1:3]
```

## Sentiment Analysis

One we have a collection of text it’s interesting to figure out what it might mean or infer - if anything at all. In text analysis and NLP (Natural Language Processing) we talk about “Bag of Words” to describe a collection or “corpus” of unstructured text. What do we do with a “bag of words” ?

1) Extract meaning from collections of text (without reading !)
2) Detect and analyze patterns in unstructured textual collections
3) Use Natural Language Processing techniques to reach conclusions
4) Discover what ideas occur in text and how they might be linked
5) Determine if the discovered patterns be used to predict behavior ?
6) Identify interesting ideas that might otherwise be ignored


Work flow

- Identify and Obtain text (e.g. websites, Twitter, Databases, PDFs, surveys)
- Create a text ”Corpus”- a structure that contains the raw text
- Apply transformations:
        Normalize case (convert to lower case)
        Remove puncutation and stopwords
        Remove domain specific stopwords
- Perform Analysis and Visualizations (word frequency, tagging, wordclouds)
- Do Sentiment Analysis

R has Packages to Help. These are just some of them:

    QDAP - Quantitative Discourse Package
    tm - text mining applications within R
    tidytext - Text Mining using ddplyr and ggplot and tidyverse tools
    SentimentAnalysis - For Sentiment Analysis

However, consider that:

    Some of these are easier to use than others
    Some can be kind of a problem to install (e.g. qdap)
    They all offer similar capabilities
    We’ll look at tidytext
    
    
### Simple Sentiment Example

```{r}
library(SentimentAnalysis)

# Create some arbitrary document
documents <- c("I hate Delta Airlines. The are always late and the gate agents are incredily rude.",
               "I enjoyed my flight from New York to Paris. Cabin service was quite good.",
               "Gee Delta. How nice of you to lose my luggage.")

# The sentiment library will let us do this right inside of R.

(sentiment <- analyzeSentiment(documents))
```
So we could force an assessment of positive or negative using one of the column ratings. 

```{r}
convertToBinaryResponse(sentiment$SentimentQDAP)

```

We'll soon be connecting the concept of sentiment analysis to our web scraping and/or API use in a moment. 

### Simple Example Web Scraping

Find the URL for Lincoln’s March 4, 1865 Speech:

```{r}
url <- "https://millercenter.org/the-presidency/presidential-speeches/march-4-1865-second-inaugural-address"
library(rvest)
lincoln_doc <- read_html(url) %>%
                    html_nodes(".view-transcript") %>%
                    html_text()
lincoln_doc
```

There are probably lots of words that don’t really “matter” or contribute to the “real” meaning of the speech. These are called "stop words". 

```{r}
word_vec <- unlist(strsplit(lincoln_doc," "))
word_vec[1:20]
```
```{r}
sort(table(word_vec),decreasing = TRUE)[1:10]
```

How do we remove all the uninteresting words ? We could do it manaully

```{r}
# Remove all punctuation marks
word_vec <- gsub("[[:punct:]]","",word_vec)
stop_words <- c("the","to","and","of","the","for","in","it",
                "a","at","this","which","by","is","an","hqs","from",
                "that","with","as")
for (ii in 1:length(stop_words)) {
    for (jj in 1:length(word_vec)) {
      if (stop_words[ii] == word_vec[jj]) {
          word_vec[jj] <- ""
} }
}
word_vec <- word_vec[word_vec != ""]
sort(table(word_vec),decreasing = TRUE)[1:10]
```

```{r}
word_vec[1:30]
```

## tidytext

So the tidytext package https://juliasilge.github.io/tidytext/ provides some accomodations to convert your body of text into individual tokens which then simplfies the removal of less meaningful words and the creation of word frequency counts. It also presents a path to doing sentiment analysis.

The first thing you do is to create a data frame where the there is one line for each body of text. In this case we have only one long string of text this will be a one line data frame.

```{r}
library(tidytext)
library(tidyr)
library(tibble)
library(dplyr)

text_df <- tibble(line = 1:length(lincoln_doc), text = lincoln_doc)
```


The next step is to breakup each of text lines (we have only 1) into invdividual rows, each with it’s own line. We also want to count the number of times that each word appears. This is known as tokenizing the data frame.

```{r}
token_text <- text_df %>%
  unnest_tokens(word, text)

# Let's now count them

token_text %>% count(word,sort=TRUE)
```

But we need to get rid of the “stop words”. It’s a good thing that the tidytext package has a way to filter out the common words that do not significantly contribute to the meaning of the overall text. The stop_words data frame is built into tidytext. Take a look to see some of the words contained therein:


```{r}
data(stop_words)

# Sample 40 random stop words

stop_words %>% sample_n(40)
```

```{r}
# Now remove stop words from the document

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

tidy_text %>% count(word,sort=TRUE)
```


```{r}
tidy_text %>% count(word,sort=TRUE)
```

```{r}
library(ggplot2)
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + 
  labs(title="Key Words in Lincoln's Speech") +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

## Back To The PubMed Example

We have around 992 abstracts that we mess with based on our work using the easyPubMed package

```{r}
library(tidytext)
library(dplyr)
text_df <- tibble(line = 1:length(my_abstracts), text = my_abstracts)
token_text <- text_df %>%
  unnest_tokens(word, text)

# Many of these words aren't helpful 
token_text %>% count(total=word,sort=TRUE)
```

```{r}
# Now remove stop words
data(stop_words)

tidy_text <- token_text %>%
  filter(!word %in% stop_words$word)

# Arrange the text by descending word frequency 

tidy_text %>%
  count(word, sort = TRUE) 
```

Some of the most frequently occurring words are in fact “dialysis”, “patients” so maybe we should consider them to be stop words also since we already know quite well that the overall theme is, well, dialysis and kidneys. There are also synonymns and abbreviations that are somewhat redundant such as “pdd”,“pd”,“hhd” so let’s eliminate them also.

```{r}
tidy_text <- token_text %>%
   filter(!word %in% c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","disease","treatment"))

tidy_text %>%
  count(word, sort = TRUE) 
```

Let’s do some plotting of these words

```{r}
library(ggplot2)
tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 60) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  labs(title="Top Words in Dialysis Abstracts") +
  coord_flip()
```


Okay, it looks like there are numbers in there which might be useful. I suspect that the “95” is probably associated with the idea of a confidence interval. But there are other references to numbers.


```{r}
grep("^[0-9]{1,3}$",tidy_text$word)[1:20]
```

```{r}
tidy_text_nonum <- tidy_text[grep("^[0-9]{1,3}$",tidy_text$word,invert=TRUE),]
```

Okay well I think maybe we have some reasonable data to examine. As you might have realized by now, manipulating data to get it “clean” can be tedious and frustrating though it is an inevitable part of the process.

```{r}
tidy_text_nonum %>%
  count(word, sort = TRUE) %>%
  filter(n > 20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  labs(title="Filter Set of Dialysis Words") +
  coord_flip()
```

##  How Do You Feel ?

The next step is to explore what some of these words might mean. The tidytext package has four dictionaries that help you figure out what sentiment is being expressed by your data frame.


```{r}
# NRC Emotion Lexicon from Saif Mohammad and Peter Turney
get_sentiments("nrc") %>% sample_n(20)
```

```{r}
# the sentiment lexicon from Bing Liu and collaborators
get_sentiments("bing") %>% sample_n(20)
```
```{r}
# Tim Loughran and Bill McDonald
get_sentiments("loughran") %>% sample_n(20)
```

```{r}
# Pull out words that correspond to joy
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

nrc_joy
```

So we will use the nrc sentiment dictionary to see the “sentiment” expressed in our abstracts.

```{r}
bing_word_counts <- tidy_text_nonum %>% 
  inner_join(get_sentiments("nrc")) %>% 
  count(word,sentiment,sort=TRUE)
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
Let’s create a word cloud

```{r}
library(wordcloud)
#

tidy_text_nonum %>%  
  count(word) %>%
  with(wordcloud(word,n,max.words=90,scale=c(4,.5),colors=brewer.pal(8,"Dark2")))
```

## Bi Grams

Let’s look at bigrams. We need to go back to the cleaned abstracts and pair words to get phrase that might be suggestive of some sentiment

```{r}
text_df <- data_frame(line = 1:length(my_abstracts), text = my_abstracts)
dialysis_bigrams <- text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

dialysis_bigrams %>%
  count(bigram, sort = TRUE)
```

```{r}
library(tidyr)
bigrams_sep <- dialysis_bigrams %>% 
  separate(bigram,c("word1","word2"),sep=" ")

stop_list <- c(stop_words$word,"dialysis","patients","home","kidney",
                       "hemodialysis","haemodialysis","treatment","patient","hhd",
                       "pd","peritoneal","hd","renal","study","care",
                       "ci","chd","nhd","esrd","lt","95","0.001")

bigrams_filtered <- bigrams_sep %>% 
  filter(!word1 %in% stop_list) %>%
  filter(!word2 %in% stop_list)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>%  count(bigram, sort = TRUE) %>% print(n=25)
```

```{r}
library(tidyquant)
bigram_counts %>%
  filter(n > 10) %>%
  ggplot(aes(x = reorder(word1, -n), y = reorder(word2, -n), fill = n)) +
    geom_tile(alpha = 0.8, color = "white") +
    scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) +
    coord_flip() +
    theme_tq() +
    theme(legend.position = "right") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    labs(x = "first word in pair",
         y = "second word in pair")
```

